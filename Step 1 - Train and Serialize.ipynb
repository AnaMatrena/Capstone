{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning notebook 1 - Train and serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this learning unit you will learn how to preserve your model so that the value it generates can be used in a separate process or program than the one in which it was fitted. There are a few different ways to do this but for this specialization we will be using the following tools:\n",
    "\n",
    "1. [pickle](https://docs.python.org/3/library/pickle.html) from the Python core\n",
    "1. [pipelines](https://scikit-learn.org/stable/modules/compose.html#pipelines-and-composite-estimators) from scikit\n",
    "\n",
    "This will be our journey:\n",
    "\n",
    "1. **Train**: We are going to first train a model on the classic titanic dataset. We will use this one because it has categorical and numeric features and missing values in both types.\n",
    "\n",
    "2. **Serialize**: Once the model has been trained as part of a pipeline, we will [serialize](https://en.wikipedia.org/wiki/Serialization) it, i.e. preserve it for later, using the [pickle](https://docs.python.org/3/library/pickle.html) package that is found in Python's core.\n",
    "\n",
    "3. **Predict on new data**: After we are confident that we can retrieve the pickled model from the disk, we will show how to prepare a brand new observation for prediction with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train your model\n",
    "\n",
    "Let's get started! We're not going to spend much time preparing the data set or working on the model performance because it's not the focus of this learning unit. So let's power through the first few steps!\n",
    "\n",
    "First we read the data set and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of a few features that don't hold anything particularly useful and take another peek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's separate the features and the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df.drop('Survived', axis=1), df.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Build the pipeline\n",
    "\n",
    "Okay, the next bit of necessary code isn't very much at all but is very dense. So let's take things one at a time to understand\n",
    "the motivation.\n",
    "\n",
    "We'll begin by confidently throwing a logistic regression at the data and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We're using a try/except block here because (SPOILER ALERT)\n",
    "# the fit function is going to fail and we want the notebook to look clean.\n",
    "# Go ahead and remove the try/except block to see the error's stack trace\n",
    "\n",
    "try:\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "except ValueError as e:\n",
    "     print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know this game - scikit classifiers don't know how to deal with non-numerical data. Since we already know about pipelines,\n",
    "let's try to put together a pipeline that has a OneHotEncoder in an attempt to deal with the non-numeric data.\n",
    "\n",
    "We'll use the [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). The default behavior is for all string columns to be dummified. `SimpleImputer` fills in missing values in the data. By default, it assumes that the missing values are NaNs, but we could specify other types of missing values with the `missing_values` keyword. With the `strategy` keyword we define how to fill in the missing values: in this case, we're replacing them with the mean value of the feature where the missing value occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_features = list(set(X_train.columns).difference(categorical_features))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy='mean'), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! Now we are using a pipeline and training a classifier without doing any explicit preprocessing of the dataset!\n",
    "\n",
    "With the pipeline we should now be able to move on and start processing new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Predict on new observations\n",
    "\n",
    "Let's construct a new observation using a protocol that is technology agnostic: [json](https://en.wikipedia.org/wiki/JSON).\n",
    "\n",
    "We will assume that a new observation has come over the wire using a transportation\n",
    "layer such as HTTP, which means that it will arrive to us as a json string (a string whose content follows the json protocol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs_str = '{\"Age\": 22.0, \"Cabin\": NaN, \"Embarked\": \"S\", \"Fare\": 7.25, \"Parch\": 0, \"Pclass\": 3, \"Sex\": \"male\", \"SibSp\": 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we've got a new observation as a json string. This is desirable because no matter what\n",
    "programming language or environment we are in, we know that there will be support for deserialization\n",
    "into a native type. In Ruby these are hashes, in javascript they are objects, and in Python they are\n",
    "dictionaries.\n",
    "\n",
    "So let's turn our json string into a dictionary - it's a great starting point to do anything we may need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs_dict = json.loads(new_obs_str)\n",
    "print('type {}'.format(type(new_obs_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so fast... scikit models don't know how to deal with dictionaries! Well, we know that when we trained the model, the pipeline took a pandas dataframe so that's what we should be passing into the pipeline's `predict_proba` as well.\n",
    "\n",
    "With that in mind, let's take a few lines of code to transform the dictionary\n",
    "into a pandas dataframe. Note that a series isn't good enough, it must be\n",
    "a full dataframe, even if it's just for a single observation.\n",
    "\n",
    "The first step is to create a dataframe with the columns in the correct order. You can get the correct order by getting the columns from the `X_train` dataframe with which the model was trained. We're passing the dictionary in a list to ensure row indexing (see discussion [here](https://stackoverflow.com/questions/17839973/constructing-dataframe-from-values-in-variables-yields-valueerror-if-using-all))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.DataFrame([new_obs_dict], columns=X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to make sure that the column types are correct, as the pipeline expects them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = obs.astype(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can call `predict_proba` and the pipeline will output the probabilities of the negative and positive class (dead or alive in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict_proba(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we are feeling pretty cool right now. We have a trained model and we can process new observations! Now that we have this under control, let's learn how to preserve our model so that we can keep this sweetness for posterity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Serialization of the necessary components\n",
    "\n",
    "Okay let's take a moment to imagine that we will need to use this model in a totally different environment than the one it was trained in.\n",
    "\n",
    "For instance, if we want to share the model with a friend: we wouldn't want them to retrain it (some models take  hours to train), and we wouldn't want to have to send them the training data (it might be very large and it might be confidential data that shouldn't be shared).\n",
    "\n",
    "Even within the same computer, we may want to use the model in a different notebook than the one it was trained in. Or in a different Python process. Or in a flask server (hint hint).\n",
    "\n",
    "What we  want to do is to save the model to the disk so that it can be transfered somewhere else and used later on.\n",
    "\n",
    "Remember that serialization is just the process of storing the state of an object so that it can be used later on. Let's think about what we need in order to be able to call `predict_proba` on a new observation. It is:\n",
    "\n",
    "1. The column names in the correct order\n",
    "1. The fitted pipeline\n",
    "1. The dtypes of the columns of the training set\n",
    "\n",
    "One at a time, let's look at serializing these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Serializing the columns in the correct order\n",
    "\n",
    "Probably the most well-known serialization format for data is json. This is great because it's robust and technology agnostic. Let's serialize the columns of the training set in the correct order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('columns.json', 'w') as fh:\n",
    "    json.dump(X_train.columns.tolist(), fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are just strings, so this simply writes the column names into a text file in the json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Serializing the fitted pipeline\n",
    "\n",
    "Now we need to serialize the fitted pipeline. Unfortunately we don't have something as clean as json for this since the pipeline is a Python object and not just text like the column names.\n",
    "\n",
    "In order to preserve the pipeline, we will need to use a library to export a Python object (the fitted pipeline) onto the disk in such a way that Python can reload it again. The most common library for serialization of Python objects is [pickle](https://docs.python.org/3/library/pickle.html) which is part of the Python standard library. However, scikit-learn comes with a version of pickle designed to save scikit-learn estimators, [joblib](http://scikit-learn.org/stable/modules/model_persistence.html), so we'll it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline, 'pipeline.pickle') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**. One thing to consider when serializing/deserializing scikit-learn estimators is that joblib/pickle don't store the definitions of the estimators (the code). This means 2 things:\n",
    "\n",
    "- All of the libraries you use to build the pipeline on your laptop need to be available (installed) in the machine where you deploy it. \n",
    "\n",
    "- All of the custom code you use in the pipeline (for example any custom transformers) needs to be defined as well when you unpickle the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Serializing the dtypes of the columns\n",
    "\n",
    "We have a similar situation with the dtypes. When you call `X_train.dtypes`, you will get a list of Python\n",
    "objects, so we have to use pickle to serialize them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dtypes.pickle', 'wb') as fh:\n",
    "    pickle.dump(X_train.dtypes, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alrighty then, we have now serialized all the necessary components\n",
    "of our model!\n",
    "\n",
    "Move on to the next notebook to see how we can deserialize and use\n",
    "all of this work in a different process (notebook)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
